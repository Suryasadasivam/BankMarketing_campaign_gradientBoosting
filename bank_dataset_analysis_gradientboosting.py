# -*- coding: utf-8 -*-
"""Bank dataset analysis_gradientBoosting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aiQvT9QqhzydeoPwrwQ6Vpqn1IYi94EN
"""

pip install pymongo

import pymongo
import pandas as pd

import numpy as np
from scipy import stats
import plotly.graph_objects as go
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import OrdinalEncoder

"""**DataBase connection**"""

client=pymongo.MongoClient("your Password")

Vb=client["bank_details"]
col=Vb["Bank_dataset"]

df=pd.read_csv("/content/Bank Marketing Campaign Dataset.csv")

cad=df.to_dict("records")
col.insert_many(cad)

data=[]
for i in col.find({},{"_id":0}):
  data.append(i)

dd=pd.DataFrame(data)
dd["conversion_status"].unique()

df1=pd.DataFrame(data)

df1

"""**DataObservation**"""

df1.columns

df1["call_frequency"].unique()

df1["call_frequency"].value_counts()

df1.info()

df1.describe()

df1.corr()

sns.heatmap(df1.corr())
plt.show()

#supervised, category
#Dependent column= conversion status
# independet column= '_id', 'occupation', 'age', 'education_level', 'marital_status' 'communication_channel', 'call_month', 'call_day', 'call_duration','call_frequency', 'previous_campaign_outcome'

"""**Hypothesis Testing**"""

continous_column=['call_duration','call_frequency']
category_column=[ 'occupation', 'age', 'education_level', 'marital_status',
       'communication_channel', 'call_month', 'call_day',
       'previous_campaign_outcome', 'conversion_status']

def central_limit_theorem(continous_column,sample_size,rage):
  result_centrallimit={}
  pop=df1[continous_column].values
  population_mean=pop.mean()
  allsample=[]
  for i in range(rage):
     sample=np.random.choice(pop,sample_size)
     allsample.append(sample.mean())
  all_sample_mean=(np.mean(allsample))
  result_centrallimit.update({'Column Name':continous_column,
                       'Population mean':population_mean,
                       'Allsample mean':all_sample_mean
                       })
  if continous_column:
    H0_accepted=0
    H0_rejected=0
    for i in range(rage):
      sample1=df1[continous_column].sample(frac=0.04)
      t_test,p_value=stats.ttest_1samp(sample1,df1[continous_column].mean())
      if p_value<0.05:
        H0_rejected+=1
      else:
        H0_accepted+=1
      if H0_accepted>H0_rejected:
        result_centrallimit.update({'Onesamplettest':' H0-There is no significant difference','Ttest':t_test,
                                    'pvalue':p_value})
      else:
        result_centrallimit.update({'Onesamplettest':' Ha-There is significant difference','Ttest':t_test,'pvalue':p_value})
    return(result_centrallimit)

res=[]
for i in continous_column:
  s=central_limit_theorem(i,50,10)
  res.append(s)
pd.DataFrame(res)

def twosamplettest(continous_column1,continous_column2,sample_size,rage):
   H0_accepted=0
   H0_rejected=0
   result={}
   allsample1=[]
   allsample2=[]
   for i in range(rage):
     sample1=df1[continous_column1].sample(frac=0.2)
     sample2=df1[continous_column2].sample(frac=0.2)
     t_test,p_value=stats.ttest_ind(sample1,sample2)
     if p_value<0.05:
        H0_rejected+=1
     else:
        H0_accepted+=1
     if H0_accepted>H0_rejected:
         result.update({
             'column':continous_column1+"&"+continous_column2,
             'twosamplettest':'H0-There is no significant difference',
             't_test_value':t_test,
              'P_value':p_value})
     else:
         result.update({
              'column':continous_column1+"&"+continous_column2,
             'twosamplettest':' Ha-There is significant difference',
              't_test_value':t_test,
              'P_value':p_value})
   if continous_column1:
     H0_accepted=0
     H0_rejected=0
     for i in range(rage):
         column1=df1[continous_column1]
         column2=df1[continous_column2]
         sample1=np.random.choice(column1,sample_size)
         sample2=np.random.choice(column2,sample_size)
         allsample1.append(sample1.mean())
         allsample2.append(sample2.mean())
     t_test,p_value=stats.ttest_ind(allsample1,allsample2)
     if p_value<0.05:
              H0_rejected+=1
     else:
            H0_accepted+=1
     if H0_accepted>H0_rejected:
              result.update({
             'column':continous_column1+"&"+continous_column2,
             'twosamplettest central':'H0-There is no significant difference',
             't_test_valuone':t_test,
             'P_valueone':p_value})
     else:
              result.update({
              'column':continous_column1+"&"+continous_column2,
             'twosamplettest central':' Ha-There is significant difference',
             't_test_valuone':t_test,
             'P_valueone':p_value})

   return result

columns=continous_column
res1=[]
for i in range (len(columns)-1):
  column1=columns[i]
  for j in range(i+1,len(columns)):
      column2=columns[j]
      j=twosamplettest(column1,column2,50,10)
      res1.append(j)
pd.DataFrame(res1)

def chi_square_test(category_column1,category_column2):
  result={}
  H0_accepted=0
  H0_rejected=0
  data1=pd.crosstab(df1[category_column1],df1[category_column2])
  observed_values=data1.values
  value=stats.chi2_contingency(observed_values)
  p_value=value[1]
  if p_value<0.05:
     H0_rejected+=1
  else:
    H0_accepted+=1
  if H0_accepted>H0_rejected:
    result.update({
        'column':category_column1+"&"+category_column2,
        "chi_square_test": "There is no relationship between two mentioned column" })
  else:
    result.update({
        'column':category_column1+"&"+category_column2,
        "chi_square_test": "There is relationship between two mentioned column" })

  return result

Category=category_column
res2=[]
for i in range(len(Category)-1):
  category1=Category[i]
  for j in range(i+1,len(Category)):
      category2=Category[j]
      chi=chi_square_test(category1,category2)
      res2.append(chi)
cat=pd.DataFrame(res2)

pd.set_option('max_colwidth', None)
cat

def annova_test(continous_column,category_column):
   result={}
   H0_accepted=0
   H0_rejected=0
   group=df1[category_column].unique()
   grp={}
   for i in group:
     grp[i]=df1[continous_column][df1[category_column]==i]
   f_value,p_value=stats.f_oneway(*grp.values())
   if p_value<0.05:
    H0_rejected+=1
   else:
     H0_accepted+=1
   if H0_accepted>H0_rejected:
     result.update({
        'column':continous_column+"&"+category_column,
        "Annova_test": "There is relationship between mentioned column"})
   else:
     result.update({
        'column':continous_column+"&"+category_column,
        "Annova_test": "There is no relationship between mentioned column"})

   return result

Category=category_column
continous=continous_column
res3=[]
for i in continous:
  for j in Category:
    ann=annova_test(i,j)
    res3.append(ann)
pd.DataFrame(res3)

"""# Preprocessing"""



category_column

encode=OrdinalEncoder()
df1['occupation']=encode.fit_transform(df1[['occupation']])
df1['education_level']=encode.fit_transform(df1[['education_level']])
df1['marital_status']=encode.fit_transform(df1[['marital_status']])
df1['communication_channel']=encode.fit_transform(df1[['communication_channel']])
df1['previous_campaign_outcome']=encode.fit_transform(df1[['previous_campaign_outcome']])
df1['conversion_status']=encode.fit_transform(df1[['conversion_status']])

df1['call_month']=encode.fit_transform(df1[['call_month']])

df1

df1["conversion_status"].value_counts()

"""# Machinelearning"""

# 1 data availability
# 2 separating independent and dependent
# 3 identifying algorithms/Model
# 4 training
# 5 evaluation

x=df1.drop('conversion_status',axis=1)
y=df1['conversion_status']

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)

from sklearn.ensemble import AdaBoostClassifier
model=AdaBoostClassifier().fit(x_train,y_train)
y_pred=model.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score

accuracy=accuracy_score(y_test,y_pred)
accuracy

from sklearn.ensemble import GradientBoostingClassifier
model1=GradientBoostingClassifier().fit(x_train,y_train)
y_pred1=model1.predict(x_test)

accuracy1=accuracy_score(y_test,y_pred1)
accuracy1

input = np.array([[7,29,3,1,2,9,27,142,4,2]])
prediction = model.predict(input)
prediction

